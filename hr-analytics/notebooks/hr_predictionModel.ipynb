{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvIbVIalursGUNkA2pcUve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moodv/data-analytics-portfolio/blob/main/hr-analytics/notebooks/hr_predictionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# HR Attrition Prediction - OPTIMIZED Pipeline (Train/Test + Full-data preds)\n",
        "# ===========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.30\n",
        "SMOTE_RATIO = 0.3\n",
        "THRESHOLD_STEPS = 100\n",
        "\n",
        "DB_CONNECTION = (\n",
        "    \"postgresql://neondb_owner:npg_ivsVpJa1bAd8@ep-cold-dust-agcio9u3-pooler.c-2.eu-central-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require\"\n",
        ")\n",
        "db_engine = create_engine(DB_CONNECTION)\n",
        "\n",
        "# ==================== 1️⃣ LOAD DATA ====================\n",
        "data_query = \"\"\"\n",
        "WITH perf AS (\n",
        "  SELECT\n",
        "    employee_id,\n",
        "    AVG(current_employee_rating) AS avg_rating,\n",
        "    AVG(engagement_score) AS avg_engagement,\n",
        "    AVG(satisfaction_score) AS avg_satisfaction,\n",
        "    AVG(work_life_balance_score) AS avg_wlb\n",
        "  FROM performance\n",
        "  GROUP BY employee_id\n",
        "),\n",
        "train AS (\n",
        "  SELECT\n",
        "    employee_id,\n",
        "    COUNT(*) AS num_trainings,\n",
        "    AVG(training_duration_days) AS avg_training_days,\n",
        "    SUM(training_cost) AS total_training_cost,\n",
        "    MAX(training_outcome) AS last_training_outcome\n",
        "  FROM training\n",
        "  GROUP BY employee_id\n",
        ")\n",
        "SELECT\n",
        "  e.employee_id,\n",
        "  e.title,\n",
        "  e.business_unit,\n",
        "  e.department_type,\n",
        "  e.division,\n",
        "  e.state,\n",
        "  e.gender_code,\n",
        "  e.race_desc,\n",
        "  e.marital_desc,\n",
        "  e.age,\n",
        "  em.start_date,\n",
        "  em.employee_status,\n",
        "  em.employee_type,\n",
        "  em.pay_zone,\n",
        "  em.employee_classification_type,\n",
        "  COALESCE(p.avg_rating,0) AS avg_rating,\n",
        "  COALESCE(p.avg_engagement,0) AS avg_engagement,\n",
        "  COALESCE(p.avg_satisfaction,0) AS avg_satisfaction,\n",
        "  COALESCE(p.avg_wlb,0) AS avg_wlb,\n",
        "  COALESCE(t.num_trainings,0) AS num_trainings,\n",
        "  COALESCE(t.avg_training_days,0) AS avg_training_days,\n",
        "  COALESCE(t.total_training_cost,0) AS total_training_cost,\n",
        "  COALESCE(t.last_training_outcome,'None') AS last_training_outcome\n",
        "FROM employees e\n",
        "LEFT JOIN employment em USING(employee_id)\n",
        "LEFT JOIN perf p USING(employee_id)\n",
        "LEFT JOIN train t USING(employee_id);\n",
        "\"\"\"\n",
        "\n",
        "raw_data = pd.read_sql(data_query, db_engine)\n",
        "print(\"✓ Data loaded:\", len(raw_data), \"rows\")\n",
        "\n",
        "# ==================== 2️⃣ CREATE TARGET ====================\n",
        "raw_data['employee_status'] = raw_data['employee_status'].astype(str)\n",
        "raw_data['attrition_flag'] = (raw_data['employee_status'].str.strip().str.lower() == 'terminated').astype(int)\n",
        "\n",
        "print(\"\\nEmployee Status Distribution:\")\n",
        "print(raw_data['employee_status'].value_counts())\n",
        "\n",
        "baseline_attrition_rate = raw_data['attrition_flag'].mean()\n",
        "print(f\"\\nBaseline Attrition Rate: {baseline_attrition_rate:.4f} ({baseline_attrition_rate*100:.2f}%)\")\n",
        "\n",
        "# ==================== 3️⃣ FEATURE ENGINEERING ====================\n",
        "raw_data['start_date'] = pd.to_datetime(raw_data['start_date'], errors='coerce')\n",
        "raw_data['tenure_years'] = ((pd.Timestamp('today') - raw_data['start_date']).dt.days / 365).fillna(0)\n",
        "\n",
        "# ==================== 4️⃣ PREPARE FEATURES ====================\n",
        "employee_ids = raw_data['employee_id'].copy()\n",
        "cols_to_drop = ['employee_id', 'start_date', 'employee_status']\n",
        "feature_data = raw_data.drop(columns=cols_to_drop)\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_columns = feature_data.select_dtypes(include='object').columns.tolist()\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    encoder = LabelEncoder()\n",
        "    feature_data[col] = encoder.fit_transform(feature_data[col].astype(str))\n",
        "    label_encoders[col] = encoder\n",
        "\n",
        "feature_data = feature_data.fillna(0)\n",
        "\n",
        "# ==================== 5️⃣ SPLIT FEATURES & TARGET ====================\n",
        "feature_matrix = feature_data.drop(columns=['attrition_flag'])\n",
        "target_vector = feature_data['attrition_flag']\n",
        "\n",
        "print(\"\\nClass Distribution (Before Resampling):\")\n",
        "print(target_vector.value_counts(normalize=True))\n",
        "\n",
        "# ==================== 6️⃣ TRAIN-TEST SPLIT ====================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    feature_matrix, target_vector, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=target_vector\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain Size: {len(X_train)} | Test Size: {len(X_test)}\")\n",
        "print(\"Test Class Distribution:\")\n",
        "print(y_test.value_counts(normalize=True))\n",
        "\n",
        "train_attrition_target = y_train.mean()\n",
        "print(f\"\\nTarget Attrition Rate (from train): {train_attrition_target:.4f}\")\n",
        "\n",
        "# ==================== 7️⃣ APPLY SMOTE & SETUP CLASS WEIGHTS ====================\n",
        "smote_sampler = SMOTE(sampling_strategy=SMOTE_RATIO, random_state=RANDOM_STATE)\n",
        "X_train_resampled, y_train_resampled = smote_sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"\\nAfter SMOTE (ratio={SMOTE_RATIO}) - Train Distribution:\")\n",
        "print(pd.Series(y_train_resampled).value_counts(normalize=True))\n",
        "\n",
        "# Calculate XGBoost scale_pos_weight\n",
        "xgb_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "print(f\"XGBoost Scale Pos Weight: {xgb_pos_weight:.2f}\")\n",
        "\n",
        "# Use non-SMOTE data for models with class weights\n",
        "X_train_balanced = X_train\n",
        "y_train_balanced = y_train\n",
        "\n",
        "# ==================== 8️⃣ TRAIN MODELS ====================\n",
        "model_configs = {\n",
        "    \"LogisticRegression\": LogisticRegression(\n",
        "        max_iter=2000,\n",
        "        class_weight='balanced',\n",
        "        solver='saga',\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        class_weight='balanced',\n",
        "        max_depth=12,\n",
        "        min_samples_split=15,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.08,\n",
        "        max_depth=5,\n",
        "        scale_pos_weight=xgb_pos_weight,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=1.0,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "}\n",
        "\n",
        "evaluation_results = []\n",
        "trained_model_dict = {}\n",
        "model_metadata = {}\n",
        "\n",
        "for model_name, model_obj in model_configs.items():\n",
        "    # Select training data\n",
        "    if model_name == \"XGBoost\":\n",
        "        X_train_current = X_train_resampled\n",
        "        y_train_current = y_train_resampled\n",
        "    else:\n",
        "        X_train_current = X_train_balanced\n",
        "        y_train_current = y_train_balanced\n",
        "\n",
        "    # Train model\n",
        "    model_obj.fit(X_train_current, y_train_current)\n",
        "    trained_model_dict[model_name] = model_obj\n",
        "\n",
        "    # Get probability predictions on test set\n",
        "    y_test_probabilities = model_obj.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Find optimal threshold that matches actual attrition rate\n",
        "    threshold_candidates = np.linspace(0.01, 0.99, THRESHOLD_STEPS)\n",
        "    optimal_threshold = 0.5\n",
        "    min_rate_diff = float('inf')\n",
        "\n",
        "    for candidate_threshold in threshold_candidates:\n",
        "        predicted_rate = (y_test_probabilities >= candidate_threshold).mean()\n",
        "        rate_difference = abs(predicted_rate - train_attrition_target)\n",
        "        if rate_difference < min_rate_diff:\n",
        "            min_rate_diff = rate_difference\n",
        "            optimal_threshold = candidate_threshold\n",
        "\n",
        "    y_test_predictions = (y_test_probabilities >= optimal_threshold).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    model_auc_score = roc_auc_score(y_test, y_test_probabilities)\n",
        "    classification_metrics = classification_report(y_test, y_test_predictions, output_dict=True, zero_division=0)\n",
        "\n",
        "    model_metadata[model_name] = {\n",
        "        'threshold': optimal_threshold,\n",
        "        'model': model_obj,\n",
        "        'probabilities': y_test_probabilities\n",
        "    }\n",
        "\n",
        "    evaluation_results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": classification_metrics.get(\"accuracy\", 0),\n",
        "        \"ROC_AUC\": model_auc_score,\n",
        "        \"Precision_Class1\": classification_metrics.get(\"1\", {}).get(\"precision\", 0),\n",
        "        \"Recall_Class1\": classification_metrics.get(\"1\", {}).get(\"recall\", 0),\n",
        "        \"F1_Class1\": classification_metrics.get(\"1\", {}).get(\"f1-score\", 0),\n",
        "        \"Optimal_Threshold\": optimal_threshold,\n",
        "        \"Predicted_Attrition_Rate\": (y_test_probabilities >= optimal_threshold).mean()\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "    print(f\"Predicted Attrition Rate: {(y_test_probabilities >= optimal_threshold).mean():.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_test_predictions, zero_division=0))\n",
        "    print(f\"ROC AUC: {model_auc_score:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_test_predictions))\n",
        "\n",
        "# ==================== 9️⃣ MODEL SELECTION ====================\n",
        "leaderboard_df = pd.DataFrame(evaluation_results).sort_values(by=\"F1_Class1\", ascending=False)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL LEADERBOARD (Sorted by F1-Score)\")\n",
        "print(\"=\"*60)\n",
        "print(leaderboard_df)\n",
        "\n",
        "best_model_name = leaderboard_df.iloc[0][\"Model\"]\n",
        "best_model_threshold = model_metadata[best_model_name]['threshold']\n",
        "best_model_obj = model_metadata[best_model_name]['model']\n",
        "print(f\"\\n✅ Selected Model: {best_model_name}\")\n",
        "print(f\"✅ Optimal Threshold: {best_model_threshold:.4f}\")\n",
        "\n",
        "# ==================== 🔎 FEATURE IMPORTANCE ====================\n",
        "if best_model_name in [\"RandomForest\", \"XGBoost\",\"LogisticRegression\"]:\n",
        "    try:\n",
        "        feature_importances = best_model_obj.feature_importances_\n",
        "        importance_df = pd.DataFrame({\n",
        "            \"feature\": feature_matrix.columns,\n",
        "            \"importance\": feature_importances\n",
        "        }).sort_values(by=\"importance\", ascending=False)\n",
        "        print(\"\\nTop 15 Most Important Features:\")\n",
        "        print(importance_df.head(15))\n",
        "    except Exception as error:\n",
        "        print(f\"Could not extract feature importances: {error}\")\n",
        "\n",
        "# ==================== 🔬 TEST SET EVALUATION ====================\n",
        "y_test_probs_final = best_model_obj.predict_proba(X_test)[:, 1]\n",
        "y_test_preds_final = (y_test_probs_final >= best_model_threshold).astype(int)\n",
        "\n",
        "# Create test evaluation dataframe\n",
        "test_results_df = X_test.copy()\n",
        "test_results_df['actual_attrition'] = y_test.values\n",
        "test_results_df['predicted_attrition'] = y_test_preds_final\n",
        "test_results_df['attrition_probability'] = y_test_probs_final\n",
        "\n",
        "# Calculate rates\n",
        "test_actual_rate = test_results_df['actual_attrition'].mean()\n",
        "test_pred_rate = test_results_df['predicted_attrition'].mean()\n",
        "test_rate_gap = abs(test_actual_rate - test_pred_rate)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Actual Attrition Rate: {test_actual_rate:.4f} ({test_actual_rate*100:.2f}%)\")\n",
        "print(f\"Predicted Attrition Rate: {test_pred_rate:.4f} ({test_pred_rate*100:.2f}%)\")\n",
        "print(f\"Rate Difference: {test_rate_gap*100:.2f}%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nFinal Classification Report:\")\n",
        "print(classification_report(y_test, y_test_preds_final, zero_division=0))\n",
        "\n",
        "# ==================== 🌍 FULL DATASET PREDICTIONS ====================\n",
        "full_feature_matrix = feature_matrix.copy()\n",
        "y_full_probs = best_model_obj.predict_proba(full_feature_matrix)[:, 1]\n",
        "y_full_preds = (y_full_probs >= best_model_threshold).astype(int)\n",
        "\n",
        "full_results_df = pd.DataFrame({\n",
        "    'employee_index': employee_ids.index,\n",
        "    'employee_id': employee_ids.values,\n",
        "    'predicted_attrition': y_full_preds,\n",
        "    'attrition_probability': y_full_probs,\n",
        "    'actual_attrition': target_vector.values\n",
        "})\n",
        "\n",
        "# Calculate rates for full dataset\n",
        "full_actual_rate = full_results_df['actual_attrition'].mean()\n",
        "full_pred_rate = full_results_df['predicted_attrition'].mean()\n",
        "full_rate_gap = abs(full_actual_rate - full_pred_rate)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FULL DATASET PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Actual Attrition Rate: {full_actual_rate:.4f} ({full_actual_rate*100:.2f}%)\")\n",
        "print(f\"Predicted Attrition Rate: {full_pred_rate:.4f} ({full_pred_rate*100:.2f}%)\")\n",
        "print(f\"Rate Difference: {full_rate_gap*100:.2f}%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ==================== 📤 EXPORT TO DATABASE ====================\n",
        "test_results_df.reset_index(drop=True, inplace=True)\n",
        "full_results_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "test_results_df.to_sql('employee_attrition_predictions_test', db_engine, if_exists='replace', index=False)\n",
        "full_results_df.to_sql('employee_attrition_predictions_full', db_engine, if_exists='replace', index=False)\n",
        "\n",
        "print('\\n✅ Tables exported to Neon Database:')\n",
        "print('   - employee_attrition_predictions_test')\n",
        "print('   - employee_attrition_predictions_full')\n",
        "print(f'\\n🎯 Pipeline Complete! Model: {best_model_name} | Threshold: {best_model_threshold:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBFoKOvydAdJ",
        "outputId": "1338c91c-699f-42d7-f643-857241fa6982"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Data loaded: 2845 rows\n",
            "\n",
            "Employee Status Distribution:\n",
            "employee_status\n",
            "Active        2458\n",
            "Terminated     387\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Baseline Attrition Rate: 0.1360 (13.60%)\n",
            "\n",
            "Class Distribution (Before Resampling):\n",
            "attrition_flag\n",
            "0    0.863972\n",
            "1    0.136028\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Train Size: 1991 | Test Size: 854\n",
            "Test Class Distribution:\n",
            "attrition_flag\n",
            "0    0.864169\n",
            "1    0.135831\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Target Attrition Rate (from train): 0.1361\n",
            "\n",
            "After SMOTE (ratio=0.3) - Train Distribution:\n",
            "attrition_flag\n",
            "0    0.769231\n",
            "1    0.230769\n",
            "Name: proportion, dtype: float64\n",
            "XGBoost Scale Pos Weight: 6.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Model: LogisticRegression\n",
            "============================================================\n",
            "Optimal Threshold: 0.6138\n",
            "Predicted Attrition Rate: 0.1300\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.87       738\n",
            "           1       0.13      0.12      0.12       116\n",
            "\n",
            "    accuracy                           0.77       854\n",
            "   macro avg       0.49      0.49      0.49       854\n",
            "weighted avg       0.76      0.77      0.76       854\n",
            "\n",
            "ROC AUC: 0.5730\n",
            "Confusion Matrix:\n",
            "[[641  97]\n",
            " [102  14]]\n",
            "\n",
            "============================================================\n",
            "Model: RandomForest\n",
            "============================================================\n",
            "Optimal Threshold: 0.4456\n",
            "Predicted Attrition Rate: 0.1475\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87       738\n",
            "           1       0.23      0.25      0.24       116\n",
            "\n",
            "    accuracy                           0.78       854\n",
            "   macro avg       0.56      0.56      0.56       854\n",
            "weighted avg       0.79      0.78      0.79       854\n",
            "\n",
            "ROC AUC: 0.6438\n",
            "Confusion Matrix:\n",
            "[[641  97]\n",
            " [ 87  29]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [13:08:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Model: XGBoost\n",
            "============================================================\n",
            "Optimal Threshold: 0.5247\n",
            "Predicted Attrition Rate: 0.1382\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87       738\n",
            "           1       0.17      0.17      0.17       116\n",
            "\n",
            "    accuracy                           0.77       854\n",
            "   macro avg       0.52      0.52      0.52       854\n",
            "weighted avg       0.77      0.77      0.77       854\n",
            "\n",
            "ROC AUC: 0.6312\n",
            "Confusion Matrix:\n",
            "[[640  98]\n",
            " [ 96  20]]\n",
            "\n",
            "============================================================\n",
            "MODEL LEADERBOARD (Sorted by F1-Score)\n",
            "============================================================\n",
            "                Model  Accuracy   ROC_AUC  Precision_Class1  Recall_Class1  \\\n",
            "1        RandomForest  0.784543  0.643772          0.230159       0.250000   \n",
            "2             XGBoost  0.772834  0.631191          0.169492       0.172414   \n",
            "0  LogisticRegression  0.766979  0.573042          0.126126       0.120690   \n",
            "\n",
            "   F1_Class1  Optimal_Threshold  Predicted_Attrition_Rate  \n",
            "1   0.239669           0.445556                  0.147541  \n",
            "2   0.170940           0.524747                  0.138173  \n",
            "0   0.123348           0.613838                  0.129977  \n",
            "\n",
            "✅ Selected Model: RandomForest\n",
            "✅ Optimal Threshold: 0.4456\n",
            "\n",
            "Top 15 Most Important Features:\n",
            "                feature  importance\n",
            "0                 title    0.156007\n",
            "20         tenure_years    0.105161\n",
            "18  total_training_cost    0.098879\n",
            "8                   age    0.084367\n",
            "3              division    0.063043\n",
            "2       department_type    0.051986\n",
            "1         business_unit    0.050997\n",
            "12           avg_rating    0.038926\n",
            "15              avg_wlb    0.037533\n",
            "4                 state    0.036509\n",
            "13       avg_engagement    0.035753\n",
            "6             race_desc    0.035147\n",
            "14     avg_satisfaction    0.035049\n",
            "17    avg_training_days    0.031585\n",
            "7          marital_desc    0.031021\n",
            "\n",
            "============================================================\n",
            "TEST SET EVALUATION\n",
            "============================================================\n",
            "Actual Attrition Rate: 0.1358 (13.58%)\n",
            "Predicted Attrition Rate: 0.1475 (14.75%)\n",
            "Rate Difference: 1.17%\n",
            "============================================================\n",
            "\n",
            "Final Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87       738\n",
            "           1       0.23      0.25      0.24       116\n",
            "\n",
            "    accuracy                           0.78       854\n",
            "   macro avg       0.56      0.56      0.56       854\n",
            "weighted avg       0.79      0.78      0.79       854\n",
            "\n",
            "\n",
            "============================================================\n",
            "FULL DATASET PREDICTIONS\n",
            "============================================================\n",
            "Actual Attrition Rate: 0.1360 (13.60%)\n",
            "Predicted Attrition Rate: 0.1677 (16.77%)\n",
            "Rate Difference: 3.16%\n",
            "============================================================\n",
            "\n",
            "✅ Tables exported to Neon Database:\n",
            "   - employee_attrition_predictions_test\n",
            "   - employee_attrition_predictions_full\n",
            "\n",
            "🎯 Pipeline Complete! Model: RandomForest | Threshold: 0.4456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjuFxFUPoCN-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}