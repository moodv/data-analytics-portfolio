{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtZyRM9gJuZk8VDNV7fOGq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moodv/data-analytics-portfolio/blob/main/hr-analytics/notebooks/hr_predictionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# HR Attrition Prediction - OPTIMIZED Pipeline (Train/Test + Full-data preds)\n",
        "# - Fixes: Calibrated probability predictions, smart threshold selection\n",
        "# - Goal: Match actual attrition rate while maximizing recall\n",
        "# ===========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.30\n",
        "NEON_CONN = (\n",
        "    \"postgresql://neondb_owner:npg_ivsVpJa1bAd8@ep-cold-dust-agcio9u3-pooler.c-2.eu-central-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require\"\n",
        ")\n",
        "ENGINE = create_engine(NEON_CONN)\n",
        "\n",
        "# ------------------  LOAD FEATURE DATASET ------------------\n",
        "query = \"\"\"\n",
        "WITH perf AS (\n",
        "  SELECT\n",
        "    employee_id,\n",
        "    AVG(current_employee_rating) AS avg_rating,\n",
        "    AVG(engagement_score) AS avg_engagement,\n",
        "    AVG(satisfaction_score) AS avg_satisfaction,\n",
        "    AVG(work_life_balance_score) AS avg_wlb\n",
        "  FROM performance\n",
        "  GROUP BY employee_id\n",
        "),\n",
        "train AS (\n",
        "  SELECT\n",
        "    employee_id,\n",
        "    COUNT(*) AS num_trainings,\n",
        "    AVG(training_duration_days) AS avg_training_days,\n",
        "    SUM(training_cost) AS total_training_cost,\n",
        "    MAX(training_outcome) AS last_training_outcome\n",
        "  FROM training\n",
        "  GROUP BY employee_id\n",
        ")\n",
        "SELECT\n",
        "  e.employee_id,\n",
        "  e.title,\n",
        "  e.business_unit,\n",
        "  e.department_type,\n",
        "  e.division,\n",
        "  e.state,\n",
        "  e.gender_code,\n",
        "  e.race_desc,\n",
        "  e.marital_desc,\n",
        "  e.age,\n",
        "  em.start_date,\n",
        "  em.employee_status,\n",
        "  em.employee_type,\n",
        "  em.pay_zone,\n",
        "  em.employee_classification_type,\n",
        "  COALESCE(p.avg_rating,0) AS avg_rating,\n",
        "  COALESCE(p.avg_engagement,0) AS avg_engagement,\n",
        "  COALESCE(p.avg_satisfaction,0) AS avg_satisfaction,\n",
        "  COALESCE(p.avg_wlb,0) AS avg_wlb,\n",
        "  COALESCE(t.num_trainings,0) AS num_trainings,\n",
        "  COALESCE(t.avg_training_days,0) AS avg_training_days,\n",
        "  COALESCE(t.total_training_cost,0) AS total_training_cost,\n",
        "  COALESCE(t.last_training_outcome,'None') AS last_training_outcome\n",
        "FROM employees e\n",
        "LEFT JOIN employment em USING(employee_id)\n",
        "LEFT JOIN perf p USING(employee_id)\n",
        "LEFT JOIN train t USING(employee_id);\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_sql(query, ENGINE)\n",
        "print(\"Loaded rows:\", len(df))\n",
        "\n",
        "# ------------------  CREATE/VERIFY TARGET ------------------\n",
        "# Create attrition flag based on explicit employee_status values (Active / Terminated)\n",
        "df['employee_status'] = df['employee_status'].astype(str)\n",
        "\n",
        "# Since employee_status only contains 'Active' or 'Terminated', set flag directly\n",
        "# This avoids accidental over-matching and ensures the flag reflects the true status values\n",
        "df['attrition_flag'] = (df['employee_status'].str.strip().str.lower() == 'terminated').astype(int)\n",
        "\n",
        "# Quick check of counts for each status\n",
        "matched = df['employee_status'].value_counts()\n",
        "print(\"employee_status value counts:\", matched)\n",
        "\n",
        "# Baseline attrition in raw dataset\n",
        "raw_attrition_rate = df['attrition_flag'].mean()\n",
        "print(f\"Raw attrition rate (from created flag): {raw_attrition_rate:.4f} ({raw_attrition_rate*100:.2f}%)\")\n",
        "\n",
        "# ------------------  DATES & TENURE ------------------\n",
        "df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
        "df['tenure_years'] = ((pd.Timestamp('today') - df['start_date']).dt.days / 365).fillna(0)\n",
        "\n",
        "# ------------------  DROP / KEEP IDENTIFIERS ------------------\n",
        "# Keep employee_id for final full-dataset export, but remove from training features\n",
        "ids = df['employee_id'].copy()\n",
        "\n",
        "drop_cols = ['employee_id', 'start_date', 'employee_status']\n",
        "# We'll drop employee_id from X but keep it elsewhere\n",
        "features_df = df.drop(columns=drop_cols)\n",
        "\n",
        "# ------------------  ENCODE / CLEAN ------------------\n",
        "# Label encode object columns (safe here because encoders are deterministic and don't use target)\n",
        "label_cols = features_df.select_dtypes(include='object').columns.tolist()\n",
        "encoders = {}\n",
        "for col in label_cols:\n",
        "    le = LabelEncoder()\n",
        "    features_df[col] = le.fit_transform(features_df[col].astype(str))\n",
        "    encoders[col] = le\n",
        "\n",
        "features_df = features_df.fillna(0)\n",
        "\n",
        "# ------------------  PREP X and y ------------------\n",
        "X = features_df.drop(columns=['attrition_flag'])\n",
        "y = features_df['attrition_flag']\n",
        "\n",
        "# Sanity check: counts\n",
        "print(\"Class distribution before resampling:\\n\", y.value_counts(normalize=True))\n",
        "\n",
        "# ------------------  STRATIFIED SPLIT (important) ------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))\n",
        "print(\"Test class distribution (true):\\n\", y_test.value_counts(normalize=True))\n",
        "\n",
        "# Store actual attrition rate for calibration later\n",
        "target_attrition_rate = y_train.mean()\n",
        "print(f\"Target attrition rate (from train): {target_attrition_rate:.4f}\")\n",
        "\n",
        "# ------------------  APPLY CLASS WEIGHTS (skip aggressive SMOTE) ------------------\n",
        "# Calculate scale_pos_weight for class imbalance in XGBoost\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "print(f\"Scale pos weight for XGBoost: {scale_pos_weight:.2f}\")\n",
        "\n",
        "# Apply conservative SMOTE only to XGBoost\n",
        "sm = SMOTE(sampling_strategy=0.3, random_state=RANDOM_STATE)\n",
        "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
        "print(\"After Conservative SMOTE - Train class distribution:\\n\", pd.Series(y_train_smote).value_counts(normalize=True))\n",
        "\n",
        "# Use non-SMOTE data for Logistic Regression and Random Forest (class weights instead)\n",
        "X_train_no_smote = X_train\n",
        "y_train_no_smote = y_train\n",
        "\n",
        "# ------------------  MODEL TRAINING ------------------\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(\n",
        "        max_iter=2000,\n",
        "        class_weight='balanced',\n",
        "        solver='saga',\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        class_weight='balanced',\n",
        "        max_depth=12,\n",
        "        min_samples_split=15,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.08,\n",
        "        max_depth=5,\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=1.0,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "}\n",
        "\n",
        "results = []\n",
        "trained_models = {}\n",
        "model_info = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Choose training data\n",
        "    if name == \"XGBoost\":\n",
        "        X_tr, y_tr = X_train_smote, y_train_smote\n",
        "    else:\n",
        "        X_tr, y_tr = X_train_no_smote, y_train_no_smote\n",
        "\n",
        "    # Train base model\n",
        "    model.fit(X_tr, y_tr)\n",
        "    trained_models[name] = model\n",
        "\n",
        "    # Get probabilities on test set\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Find threshold that matches actual attrition rate on test set\n",
        "    # This ensures: predicted_rate ≈ actual_rate\n",
        "    thresholds_to_try = np.linspace(0.01, 0.99, 100)\n",
        "    best_threshold = 0.5\n",
        "    best_diff = float('inf')\n",
        "\n",
        "    for thresh in thresholds_to_try:\n",
        "        pred_rate = (y_prob >= thresh).mean()\n",
        "        diff = abs(pred_rate - target_attrition_rate)\n",
        "        if diff < best_diff:\n",
        "            best_diff = diff\n",
        "            best_threshold = thresh\n",
        "\n",
        "    y_pred = (y_prob >= best_threshold).astype(int)\n",
        "\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "    model_info[name] = {\n",
        "        'threshold': best_threshold,\n",
        "        'model': model,\n",
        "        'prob': y_prob\n",
        "    }\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": report.get(\"accuracy\", 0),\n",
        "        \"ROC_AUC\": auc,\n",
        "        \"Precision (1)\": report.get(\"1\", {}).get(\"precision\", 0),\n",
        "        \"Recall (1)\": report.get(\"1\", {}).get(\"recall\", 0),\n",
        "        \"F1 (1)\": report.get(\"1\", {}).get(\"f1-score\", 0),\n",
        "        \"Optimal_Threshold\": best_threshold,\n",
        "        \"Predicted_Rate\": (y_prob >= best_threshold).mean()\n",
        "    })\n",
        "\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(f\"Optimal Threshold: {best_threshold:.4f}\")\n",
        "    print(f\"Predicted attrition rate at threshold: {(y_prob >= best_threshold).mean():.4f}\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "    print(\"ROC AUC:\", auc)\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "leaderboard = pd.DataFrame(results).sort_values(by=\"F1 (1)\", ascending=False)\n",
        "print(\"\\nModel leaderboard (sorted by F1-score):\\n\")\n",
        "print(leaderboard)\n",
        "\n",
        "# ------------------  SELECT FINAL MODEL ------------------\n",
        "best_model_name = leaderboard.iloc[0][\"Model\"]\n",
        "best_threshold = model_info[best_model_name]['threshold']\n",
        "final_model = model_info[best_model_name]['model']\n",
        "print(f\"\\n Final Model Selected: {best_model_name}\")\n",
        "print(f\" Optimal Threshold: {best_threshold:.4f}\")\n",
        "\n",
        "# ------------------  FEATURE IMPORTANCE (if available) ------------------\n",
        "feat_imp = None\n",
        "if best_model_name in [\"RandomForest\", \"XGBoost\"]:\n",
        "    try:\n",
        "        base_model = trained_models[best_model_name]\n",
        "        importances = base_model.feature_importances_\n",
        "        feat_imp = pd.DataFrame({\"feature\": X.columns, \"importance\": importances}).sort_values(by=\"importance\", ascending=False)\n",
        "        print(\"\\nTop feature importances:\\n\", feat_imp.head(15))\n",
        "    except Exception as e:\n",
        "        print(\"Could not extract feature importances:\", e)\n",
        "\n",
        "# ------------------  EVALUATION ON TEST SET ------------------\n",
        "# Get probabilities and apply optimized threshold\n",
        "y_prob_test = final_model.predict_proba(X_test)[:, 1]\n",
        "y_pred_final = (y_prob_test >= best_threshold).astype(int)\n",
        "\n",
        "# Create test results DF for export\n",
        "X_test_eval = X_test.copy()\n",
        "X_test_eval['actual_attrition'] = y_test.values\n",
        "X_test_eval['predicted_attrition'] = y_pred_final\n",
        "X_test_eval['attrition_probability'] = y_prob_test\n",
        "\n",
        "# Sanity checks: test actual vs predicted rates\n",
        "test_actual_rate = X_test_eval['actual_attrition'].mean()\n",
        "test_pred_rate = X_test_eval['predicted_attrition'].mean()\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Test actual attrition rate: {test_actual_rate:.4f} ({test_actual_rate*100:.2f}%)\")\n",
        "print(f\"Test predicted attrition rate: {test_pred_rate:.4f} ({test_pred_rate*100:.2f}%)\")\n",
        "print(f\"Rate difference: {abs(test_actual_rate - test_pred_rate)*100:.2f}%\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "print(\"Final Test Set Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_final, zero_division=0))\n",
        "\n",
        "# ------------------  FULL-DATA PREDICTIONS ------------------\n",
        "# Run predictions on the full original X\n",
        "full_X = X.copy()\n",
        "y_prob_full = final_model.predict_proba(full_X)[:, 1]\n",
        "y_pred_full = (y_prob_full >= best_threshold).astype(int)\n",
        "\n",
        "full_preds = pd.DataFrame({\n",
        "    'employee_index': ids.index,\n",
        "    'employee_id': ids.values\n",
        "})\n",
        "full_preds['predicted_attrition'] = y_pred_full\n",
        "full_preds['attrition_probability'] = y_prob_full\n",
        "\n",
        "# Attach actual (from created flag) for full dataset\n",
        "full_preds['actual_attrition'] = y.values\n",
        "\n",
        "# Include top 15 features in the full_preds DataFrame\n",
        "if feat_imp is not None:\n",
        "    top_15_features = feat_imp['feature'].head(15).tolist()\n",
        "    # Ensure the features are present in the original features_df before adding to full_preds\n",
        "    features_to_add = [f for f in top_15_features if f in features_df.columns]\n",
        "    for feature in features_to_add:\n",
        "        # Use the original feature data from features_df, indexed by employee_index\n",
        "        full_preds[feature] = features_df.loc[full_preds['employee_index'], feature].values\n",
        "\n",
        "\n",
        "full_actual_rate = full_preds['actual_attrition'].mean()\n",
        "full_pred_rate = full_preds['predicted_attrition'].mean()\n",
        "print(f\"Full dataset actual attrition rate (created flag): {full_actual_rate:.4f} ({full_actual_rate*100:.2f}%)\")\n",
        "print(f\"Full dataset predicted attrition rate: {full_pred_rate:.4f} ({full_pred_rate*100:.2f}%)\")\n",
        "print(f\"Rate difference: {abs(full_actual_rate - full_pred_rate)*100:.2f}%\")\n",
        "\n",
        "# ------------------  EXPORT to NEON ------------------\n",
        "# Export test-eval results and full-dataset predictions as two separate tables\n",
        "X_test_eval.reset_index(drop=True, inplace=True)\n",
        "full_preds.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Ensure column names are valid SQL identifiers (lowercase)\n",
        "full_preds.columns = full_preds.columns.str.lower()\n",
        "X_test_eval.columns = X_test_eval.columns.str.lower()\n",
        "\n",
        "\n",
        "X_test_eval.to_sql('employee_attrition_predictions_test', ENGINE, if_exists='replace', index=False)\n",
        "full_preds.to_sql('employee_attrition_predictions_full', ENGINE, if_exists='replace', index=False)\n",
        "\n",
        "print('\\n Exported tables to Neon:')\n",
        "print('- employee_attrition_predictions_test (test subset evaluation)')\n",
        "print('- employee_attrition_predictions_full (predictions for full dataset)')\n",
        "print(f'\\n Optimization Complete! Threshold: {best_threshold:.4f} | Model: {best_model_name}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBFoKOvydAdJ",
        "outputId": "9f82fd8c-dcd5-40b7-9940-05a75c915d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded rows: 2845\n",
            "employee_status value counts: employee_status\n",
            "Active        2458\n",
            "Terminated     387\n",
            "Name: count, dtype: int64\n",
            "Raw attrition rate (from created flag): 0.1360 (13.60%)\n",
            "Class distribution before resampling:\n",
            " attrition_flag\n",
            "0    0.863972\n",
            "1    0.136028\n",
            "Name: proportion, dtype: float64\n",
            "Train size: 1991 Test size: 854\n",
            "Test class distribution (true):\n",
            " attrition_flag\n",
            "0    0.864169\n",
            "1    0.135831\n",
            "Name: proportion, dtype: float64\n",
            "Target attrition rate (from train): 0.1361\n",
            "Scale pos weight for XGBoost: 6.35\n",
            "After Conservative SMOTE - Train class distribution:\n",
            " attrition_flag\n",
            "0    0.769231\n",
            "1    0.230769\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== LogisticRegression =====\n",
            "Optimal Threshold: 0.6138\n",
            "Predicted attrition rate at threshold: 0.1300\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.87       738\n",
            "           1       0.13      0.12      0.12       116\n",
            "\n",
            "    accuracy                           0.77       854\n",
            "   macro avg       0.49      0.49      0.49       854\n",
            "weighted avg       0.76      0.77      0.76       854\n",
            "\n",
            "ROC AUC: 0.57304223904308\n",
            "Confusion Matrix:\n",
            " [[641  97]\n",
            " [102  14]]\n",
            "\n",
            "===== RandomForest =====\n",
            "Optimal Threshold: 0.4456\n",
            "Predicted attrition rate at threshold: 0.1475\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87       738\n",
            "           1       0.23      0.25      0.24       116\n",
            "\n",
            "    accuracy                           0.78       854\n",
            "   macro avg       0.56      0.56      0.56       854\n",
            "weighted avg       0.79      0.78      0.79       854\n",
            "\n",
            "ROC AUC: 0.6437716101298944\n",
            "Confusion Matrix:\n",
            " [[641  97]\n",
            " [ 87  29]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:29:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== XGBoost =====\n",
            "Optimal Threshold: 0.5247\n",
            "Predicted attrition rate at threshold: 0.1382\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87       738\n",
            "           1       0.17      0.17      0.17       116\n",
            "\n",
            "    accuracy                           0.77       854\n",
            "   macro avg       0.52      0.52      0.52       854\n",
            "weighted avg       0.77      0.77      0.77       854\n",
            "\n",
            "ROC AUC: 0.6311910101859639\n",
            "Confusion Matrix:\n",
            " [[640  98]\n",
            " [ 96  20]]\n",
            "\n",
            "Model leaderboard (sorted by F1-score):\n",
            "\n",
            "                Model  Accuracy   ROC_AUC  Precision (1)  Recall (1)  \\\n",
            "1        RandomForest  0.784543  0.643772       0.230159    0.250000   \n",
            "2             XGBoost  0.772834  0.631191       0.169492    0.172414   \n",
            "0  LogisticRegression  0.766979  0.573042       0.126126    0.120690   \n",
            "\n",
            "     F1 (1)  Optimal_Threshold  Predicted_Rate  \n",
            "1  0.239669           0.445556        0.147541  \n",
            "2  0.170940           0.524747        0.138173  \n",
            "0  0.123348           0.613838        0.129977  \n",
            "\n",
            "✅ Final Model Selected: RandomForest\n",
            "✅ Optimal Threshold: 0.4456\n",
            "\n",
            "Top feature importances:\n",
            "                 feature  importance\n",
            "0                 title    0.156007\n",
            "20         tenure_years    0.105161\n",
            "18  total_training_cost    0.098879\n",
            "8                   age    0.084367\n",
            "3              division    0.063043\n",
            "2       department_type    0.051986\n",
            "1         business_unit    0.050997\n",
            "12           avg_rating    0.038926\n",
            "15              avg_wlb    0.037533\n",
            "4                 state    0.036509\n",
            "13       avg_engagement    0.035753\n",
            "6             race_desc    0.035147\n",
            "14     avg_satisfaction    0.035049\n",
            "17    avg_training_days    0.031585\n",
            "7          marital_desc    0.031021\n",
            "\n",
            "============================================================\n",
            "Test actual attrition rate: 0.1358 (13.58%)\n",
            "Test predicted attrition rate: 0.1475 (14.75%)\n",
            "Rate difference: 1.17%\n",
            "============================================================\n",
            "\n",
            "Final Test Set Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87       738\n",
            "           1       0.23      0.25      0.24       116\n",
            "\n",
            "    accuracy                           0.78       854\n",
            "   macro avg       0.56      0.56      0.56       854\n",
            "weighted avg       0.79      0.78      0.79       854\n",
            "\n",
            "Full dataset actual attrition rate (created flag): 0.1360 (13.60%)\n",
            "Full dataset predicted attrition rate: 0.1677 (16.77%)\n",
            "Rate difference: 3.16%\n",
            "\n",
            "📤 Exported tables to Neon:\n",
            "- employee_attrition_predictions_test (test subset evaluation)\n",
            "- employee_attrition_predictions_full (predictions for full dataset)\n",
            "\n",
            "✅ Optimization Complete! Threshold: 0.4456 | Model: RandomForest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjuFxFUPoCN-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}